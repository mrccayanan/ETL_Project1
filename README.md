# ETL_Project

Data Cleanup & Analysis , Project Report
Once I've identified my datasets, perform ETL on the data.

Extract:
The sources of data that you will extract from(Datasets were Extracted as CSV files , read & manipulated by using Pandas , Data stored in Postrgres databases managed by PgAdmin4):

Transform:
The type of transformation needed for this data (cleaning, joining, filtering, aggregating, etc).
Created a filtered dataframe from specific columns

Renamed the column headers

Cleaned the data by dropping duplicates and setting the index

Attempted to combine both csv files with the INNER JOINS clause in PostgresSQL

Load:
The type of final production database to load the data into (relational or non-relational).
Created database , table schema , queries with Pgadmin4 & stored in PostgresSQL / Loaded data into PostgresSQL database from Pandas

The final tables or collections that will be used in the production database.
Loaded DataFrames into PostgresSQL Database as Traffic_db

Connected to Network with Mongod

Data Analytic Tools used for Project 1
A combination of Jupyter Notebook , Python, Pandas , PostgresSQL , PgAdmin4 , MongoD, MongoDB Compass, Flask , Html were used for this project.
